### 爬虫概念

​	什么是爬虫？写程序，然后去互联网上抓取数据的过程
	互联网：网，有好多的a链接组成的，网的节点就是每一个a链接   url（统一资源定位符）
	哪些语言可以实现爬虫？
		（1）php，可以做，号称世界上最优美的语言，多进程、多线程支持的不好
		（2）java，也可以做爬虫，人家做的很好，最主要的竞争对手，代码臃肿，重构成本大
		（3）c、c++，是你能力的体现，不是良好的选择
		（4）python，世界上最美丽的语言，语法简单、代码优美，学习成本低，支持的模块多，非常强大的框架  scrapy
	通用爬虫、聚焦爬虫
		通用爬虫：百度、360、搜狐、谷歌、必应。。。
		原理：
			（1）抓取网页
			（2）采集数据
			（3）数据处理
			（4）提供检索服务
		爬虫：baiduspider
		通用爬虫如何抓取新网站？
		（1）主动提交url
		（2）设置友情链接
		（3）百度会和DNS服务商合作，抓取新网站
		检索排名
			竞价排名
			根据pagerank值、访问量、点击量（SEO）
		robots.txt
			如果不想让***爬取，可以编写robots.txt，这个协议是口头上的协议
		聚焦爬虫
			根据特定的需求，抓取指定的数据
			思路？
				代替浏览器上网
				网页的特点：
				（1）网页都有自己唯一的url
				（2）网页内容都是html结构的
				（3）使用的都是http、https协议
			爬取步骤
			（1）给一个url
			（2）写程序，模拟浏览器访问url
			（3）解析内容，提取数据
	环境：
		windows环境、linux环境
		Python3.6   64位的
		sublime  pycharm

### http协议

​	什么是协议？双方规定的传输形式
	http协议：网站原理，应用层的协议  ftp(21)
	http(80)\https(443)   ssh(22)  mysql(3306)  redis(6379)
	mongo(27017)
	https://www.cnblogs.com/wqhwe/p/5407468.html
	HTTPS和HTTP的区别主要如下：
　　1、https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。
　　2、http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。
　　3、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。
　　4、http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。
	公钥、私钥
	加密：密钥   解密：密钥   
	对称加解密
	非对称加解密
		服务端：公钥、私钥
	http协议详解（图解http协议）
	https://www.cnblogs.com/10158wsj/p/6762848.html
		http请求
			包含：请求行、请求头、请求内容
		请求行：
			get、post，以及区别
		常见请求头：
			accept:浏览器通过这个头告诉服务器，它所支持的数据类型
		　　Accept-Charset: 浏览器通过这个头告诉服务器，它支持哪种字符集
		　　Accept-Encoding：浏览器通过这个头告诉服务器，支持的压缩格式
		　　Accept-Language：浏览器通过这个头告诉服务器，它的语言环境
		　　Host：浏览器通过这个头告诉服务器，想访问哪台主机
		　　If-Modified-Since: 浏览器通过这个头告诉服务器，缓存数据的时间
		　　Referer：浏览器通过这个头告诉服务器，客户机是哪个页面来的  防盗链
		　　Connection：浏览器通过这个头告诉服务器，请求完后是断开链接还是何持链接
			X-Requested-With: XMLHttpRequest  代表通过ajax方式进行访问
		http响应头部信息
			Location: 服务器通过这个头，来告诉浏览器跳到哪里
		　　Server：服务器通过这个头，告诉浏览器服务器的型号
		　　Content-Encoding：服务器通过这个头，告诉浏览器，数据的压缩格式
		　　Content-Length: 服务器通过这个头，告诉浏览器回送数据的长度
		　　Content-Language: 服务器通过这个头，告诉浏览器语言环境
		　　Content-Type：服务器通过这个头，告诉浏览器回送数据的类型
		　　Refresh：服务器通过这个头，告诉浏览器定时刷新
		　　Content-Disposition: 服务器通过这个头，告诉浏览器以下载方式打数据
		　　Transfer-Encoding：服务器通过这个头，告诉浏览器数据是以分块方式回送的
		　　Expires: -1  控制浏览器不要缓存
		　　Cache-Control: no-cache  
		　　Pragma: no-cache

### urllib库

模拟浏览器发送请求的库，Python自带
	Python2：urllib  urllib2
	Python3: urllib.request   urllib.parse
	字符串==》字节类型之间的转化
		encode()    字符串==》字节类型
			如果小括号里面不写参数，默认是utf8
			如果写，你就写 gbk
		decode()    字节类型==》字符串
			如果不写   默认utf8
			如果写   写 gbk
	urllib.request
		urlopen(url)   发送请求
		urlretrieve(url, image_path)  下载
	urllib.parse
		quote   url编码函数，将中文进行转化为%xxx
		unquote url解码函数，将%xxx转化为指定字符
		urlencode  给一个字典，将字典拼接为query_string,并且实现了编码的功能
	response
		read()       读取相应内容，内容是字节类型
		geturl()     获取请求的url
		getheaders() 获取头部信息，列表里面有元组
		getcode()    获取状态码
		readlines()  按行读取，返回列表，都是字节类型

### urllib的cookie登录

​	# 创建一个cookiejar对象
	cj = http.cookiejar.CookieJar()
	# 通过cookiejar创建一个handler
	handler = urllib.request.HTTPCookieProcessor(cj)
	# 根据handler创建一个opener
	opener = urllib.request.build_opener(handler)
	再往下所有的操作都是用opener.open方法去发送请求，因为这里面携带cookie

### 正则表达式解析

**详细的正则可以查看正则的文档**
		单字符：
			. : 除换行以外所有字符
			[] ：`[aoe] [a-w] `匹配集合中任意一个字符
			\d ：数字  [0-9]
			\D : 非数字
			\w ：数字、字母、下划线、中文
			\W : 非\w
			\s ：所有的空白字符
			\S : 非空白
		数量修饰：
			* : 任意多次  >=0
			+ : 至少1次   >=1
			? : 可有可无  0次或者1次
			{m} ：固定m次
			{m,} ：至少m次
			{m,n} ：m-n次
		边界：
			\b \B 
			$ : 以某某结尾 
			^ : 以某某开头
		分组：
			(ab){3}
			(){4}  视为一个整体
			()     子模式\组模式   \1  \2
		贪婪模式
			.*?  .+?
		re.I : 忽略大小写
		re.M ：多行匹配
		re.S ：单行匹配

​		match\search\findall
		re.sub(正则表达式, 替换内容, 字符串)

### bs4解析

​	BeautifulSoup
	需要安装：pip install bs4
		bs4在使用时候需要一个第三方库，把这个库也安装一下
		pip install lxml
	简单使用：
		说明：选择器，jquery
		from bs4 import BeautifulSoup
		使用方式：可以将一个html文档，转化为指定的对象，然后通过对象的方法或者属性去查找指定的内容
		（1）转化本地文件：
			soup = BeautifulSoup(open('本地文件'), 'lxml')
		（2）转化网络文件：
			soup = BeautifulSoup('字符串类型或者字节类型', 'lxml')
	（1）根据标签名查找
		soup.a   只能找到第一个符合要求的标签
	（2）获取属性
		soup.a.attrs  获取所有的属性和值，返回一个字典
		soup.a.attrs['href']   获取href属性
		soup.a['href']   也可简写为这种形式
	（3）获取内容
		soup.a.string
		soup.a.text
		soup.a.get_text()
		如果标签还有标签，那么string获取到的结果为None，而其它两个，可以获取文本内容
	（4）find
		soup.find('a')   找到第一个符号要求的a
		soup.find('a', title="xxx")
		soup.find('a', alt="xxx")
		soup.find('a', class_="xxx")
		soup.find('a', id="xxx")
		find方法不仅soup可以调用，普通的div对象也能调用，会去指定的div里面去查找符合要求的节点
		find找到的都是第一个符合要求的标签
	（5）find_all
		soup.find_all('a')
		soup.find_all(['a', 'b'])
		soup.find_all('a', limit=2)  限制前两个
	（6）select
		根据选择器选择指定的内容
		常见的选择器：标签选择器、类选择器、id选择器、组合选择器、层级选择器、伪类选择器、属性选择器
		a  
		.dudu
		#lala
		a, .dudu, #lala, .meme
		div .dudu #lala .meme .xixi  下面好多级
		div > p > a > .lala   只能是下面一级
		input[name='lala']
		select选择器返回永远是列表，需要通过下标提取指定的对象，然后获取属性和节点
		该方法也可以通过普通对象调用，找到都是这个对象下面符合要求的所有节点

### xpath解析

​	pip install lxml
	什么是xpath？
		xml是用来存储和传输数据使用的
		和html的不同有两点：
		（1）html用来显示数据，xml是用来传输数据
		（2）html标签是固定的，xml标签是自定义的
		xpath用来在xml中查找指定的元素，它是一种路径表达式
		常用的路径表达式
		// : 不考虑位置的查找
		./ : 从当前节点开始往下查找
		@ : 选取属性
实例：
/bookstore/book  选取根节点bookstore下面所有直接子节点book
//book           选取所有book
bookstore//book  查找bookstore下面所有的book
/bookstore/book[1]  bookstore里面的第一个book
/bookstore/book[last()]  bookstore里面的最后一个book
/bookstore/book[position()<3]  前两个book
//title[@lang]    所有的带有lang属性的title节点
//title[@lang='eng']  所有的lang属性值为eng的title节点任何元素节点

属性定位
	//input[@id="kw"]
	//input[@class="bg s_btn"]
层级定位
索引定位
	//div[@id="head"]/div/div[2]/a[@class="toindex"]
	【注】索引从1开始
	//div[@id="head"]//a[@class="toindex"]
	【注】双斜杠代表下面所有的a节点，不管位置
逻辑运算
	//input[@class="s_ipt" and @name="wd"]
模糊匹配
	contains
		//input[contains(@class, "s_i")]
		所有的input，有class属性，并且属性中带有s_i的节点
		//input[contains(text(), "爱")]
	starts-with
		//input[starts-with(@class, "s")]
		所有的input，有class属性，并且属性以s开头
取文本
	//div[@id="u1"]/a[5]/text()  获取节点内容
	//div[@id="u1"]//text()      获取节点里面不带标签的所有内容
	直接将所有的内容拼接起来返回给你
	ret = tree.xpath('//div[@class="song"]')
	string = ret[0].xpath('string(.)')
	print(string.replace('\n', '').replace('\t', ''))
取属性
	//div[@id="u1"]/a[5]/@href
代码中使用xpath
	from lxml import etree
	两种方式使用：将html文档变成一个对象，然后调用对象的方法去查找指定的节点
	（1）本地文件
		tree = etree.parse(文件名)
	（2）网络文件
		tree = etree.HTML(网页字符串)
	ret = tree.xpath(路径表达式)
	【注】ret是一个列表

### jsonpath解析

​	jsonpath: 用来解析json数据使用的
	Python处理json格式用到的函数
		import json
		json.dumps() : 将字典或者列表转化为json格式的字符串
		json.loads() ：将json格式字符串转化为python对象
		json.dump() : 将字典或者列表转化为json格式字符串并且写入到文件中
		json.load() ：从文件中读取json格式字符串，转化为python对象
	前端处理：
		将json格式字符串转化为js对象
		JSON.parse('json格式字符串')
		eval('(' + json格式字符串 + ')')
	安装：
		pip install lxml
		pip install jsonpath
	http://blog.csdn.net/luxideyao/article/details/77802389
	xpath和jsonpath的对比
		/     $   根元素
		.     @   当前元素
		/     .   子元素
		//    ..  任意位置查找
		*     *   通配符
		[]    ?() 过滤
		xpath 索引下标从1开始
		jsonpath 索引下标从0开始

### selenium和PhantomJS

​	selenium是什么？
		是一个Python的一个第三方库，对外提供的接口可以操作你的浏览器，然后让浏览器完成自动化的操作

​	PhantomJS是什么？

​		是一款浏览器，是无界面浏览器

​	使用selenium
		安装：pip install selenium
		操作谷歌浏览器，首先必须有谷歌浏览器的一个驱动
	谷歌驱动下载地址
		http://chromedriver.storage.googleapis.com/index.html
	谷歌驱动和谷歌浏览器版本关系映射表
		http://blog.csdn.net/huilan_same/article/details/51896672
	代码操作
		from selenium import webdriver
		browser = webdriver.Chrome(path)
		browser.get()
		使用下面的方法，查找指定的元素进行操作即可
		find_element_by_id         根据id找节点
		find_elements_by_name      根据name找
		find_elements_by_xpath     根据xpath查找
		find_elements_by_tag_name  根据标签名找
		find_elements_by_class_name  根据class名字查找
		find_elements_by_css_selector  根据选择器查找
		find_elements_by_link_text   根据链接内容查找
		get\send_keys\click
	

### Headless Chrome

​	引入无界面谷歌浏览器
	因为phantomjs现在都不维护了
	mac、linux，版本号在59+以上，才支持这种模式
	windows，要求版本号在60+以上，才支持这种模式
	from selenium.webdriver.chrome.options import Options
	chrome_options = Options()
	chrome_options.add_argument('--headless')
	chrome_options.add_argument('--disable-gpu')

### requests使用

​	安装：pip install requests
	官方文档：
		http://cn.python-requests.org/zh_CN/latest/
	用来做什么？
		和urllib是同一个位置的
	get请求
	定制头部
		data：是一个原生字典即可
		requests.get(url, headers=headers, params=data)
	响应对象
		r.text  字符串形式查看响应
		r.content 字节类型查看响应
		r.encoding 查看或者设置编码类型
		r.status_code 查看状态码
		r.headers  查看响应头部
		r.url      查看所请求的url
		r.json()   查看json格式数据
	post请求
		formdata: 是一个原生字典即可
		r = requests.post(url=url, headers=headers, data=formdata)
	ajax、get、post
		和上面的是一样的
	代理
		r = requests.get(url, headers=headers, proxies=proxies)

### requests的cookie登录

​		s = requests.session()
		s.post()
		s.get()

​		用此方式发起请求，自动携带cookie

### tesseract简介

​	光学识别，但是不要对它期望太高。只能识别简单的
	代码测试
		pip install pytesseract
		pip install pillow
	# 转化为灰度图片
	img = img.convert('L')
	img.show()
	# 二值化处理
	threshold = 140
	table = []
	for i in range(256):
	    if i < threshold:
	        table.append(0)
	    else:
	        table.append(1)
	out = img.point(table, '1')
	out.show()
	img = img.convert('RGB')
	enhancer = ImageEnhance.Color(img)
	enhancer = enhancer.enhance(0)
	enhancer = ImageEnhance.Brightness(enhancer)
	enhancer = enhancer.enhance(2)
	enhancer = ImageEnhance.Contrast(enhancer)
	enhancer = enhancer.enhance(8)
	enhancer = ImageEnhance.Sharpness(enhancer)
	img = enhancer.enhance(20)

### 线程与多线程爬取

​	（1）引入
			多任务，多个任务同时进行，多进程、多线程
			sublime、录屏、vnc服务器
			word，编辑、检查（多线程）
			qq，语音、视频、发送消息（多线程）
		程序中、代码中
	（2）创建线程Thread
		面向过程
			t = threading.Thread(target=xxx, name=xxx, args=(xx, xx))
			target: 线程启动之后要执行的函数
			name: 线程的名字  
			获取线程名字 ：threading.current_thread().name
			args: 主线程向子线程传递参数
			t.start() : 启动线程
			t.join() : 让主线程等待子线程结束
		面向对象
			定义一个类，继承自threading.Thread,重写一个方法run方法，需要线程名字、传递参数，重写构造方法，在重写构造方法的时候，一定要注意手动调用父类的构造方法。
		线程同步
			线程之间共享全局变量，很容易发生数据的紊乱问题，这个时候要使用线程锁，抢，谁抢到，谁上锁之后，谁就先使用
			创建锁
				suo = threading.Lock()
			上锁
				suo.acquire()
			释放锁
				suo.release()
		队列（queue）
			下载线程
			解析线程，通过队列进行交互
			q = Queue(5)
			q.put('xxx')   如果队列满，程序卡在这里等待
			q.put(xxx, False) 如果队列满，程序直接报错
			q.put(xxx, True, 3) 如果队列满，程序等待3s在报错
			获取数据
			q.get()   如果队列为空，程序卡在这里等待
			q.get(False) 如果队列为空，程序直接报错
			q.get(True, 3) 如果队列为空，程序等待3s报错
			q.empty()  判断队列是否为空
			q.full()   判断队列是否已满
			q.qsize()  获取队列长度
	多线程爬虫
	分析：
		两类线程：下载（3）、解析（3）
		内容队列：下载线程往队列中put数据，解析线程从队列get数据
		url队列：下载线程从url队列get数据
		写数据：上锁

### scrapy框架的使用

​	1、scrapy是什么？
		是Python的一个爬虫框架，非常出名、非常强悍，你是框架，学的就是用法，当然，底层肯定使用了多进程、多线程、队列等技术
	安装：pip install scrapy
	框架的介绍
		框架有5部分组成
		引擎、下载器、spiders、调度器（schedule）、管道（pipeline）
		我们的代码写到spiders、管道中，spiders里面我们要实现文件内容解析，链接提取，管道：数据是保存到文件中、mysql中、MongoDB中？
	简单使用：
		（1）创建项目
			scrapy startproject firstblood
		（2）认识目录结构
			firstblood
				firstblood           真正的项目文件
					__pycache__      缓存文件
					spiders          爬虫文件存放的地方
						__pycache__
						__init__.py
						lala.py      爬虫文件（*）
					__init__.py      包的标志
					items.py         定义数据结构的地方（*）
					middlewares.py   中间件
					pipelines.py     管道文件（*）
					settings.py      配置文件（*）
				scrapy.cfg           不用管
		（3）生成爬虫文件
			cd firstblood
			scrapy    genspider    爬虫名称     网址域名(如www.xxx.com)
			name: 爬虫名字
			allowed_domains: 允许的域名
			start_urls: 起始url
			parse: 自动回调的解析内容函数
		（4）认识response对象
			程序跑起来：
				cd firstblood/firstblood/spiders
				scrapy crawl 爬虫名称
				(问题1)pywin32安装一下，注意版本
				(问题2)取消遵从robots协议
					settings.py中第22行
				(问题3)修改UA头部信息
					settings.py中第19行
			response的常用方法和属性
				text: 字符串类型
				body: 字节类型
				xpath(): scrapy内部已经集成了xpath，直接使用即可，此xpath非彼xpath，略有不同
		（5）执行输出指定格式
			scrapy crawl qiubai -o qiubai.json
			scrapy crawl qiubai -o qiubai.xml
			scrapy crawl qiubai -o qiubai.csv
2、scrapy shell
	response属性、方法
	selector对象
	item对象
3、yield item和请求

### crawlspider的使用

​	crawlspider是什么？
		也是一个spider，是Spider的一个子类，所以其功能要比Spider要强大
		多的一个功能是：提取链接的功能，根据一定的规则，提取指定的链接
	（1）.创建scrapy工程：scrapy startproject projectName
	（2）.创建爬虫文件：scrapy genspider -t crawl spiderName www.xxx.com
	链接提取器
		LinkExtractor(
			allow=xxx,   # 正则表达式，要（*）
			deny=xxx,    # 正则表达式，不要这个
			restrict_xpaths=xxx,  # xpath路径（*）
			restrict_css=xxx, # 选择器（*）
			deny_domains=xxx, # 不允许的域名
		)
	用法演示
		scrapy shell http://www.xxxx.com/movie/(爬取的起始网站链接)
		from scrapy.linkextractors import  LinkExtractor
		通过正则提取链接
			links = LinkExtractor(allow=r'/movie/\?page=\d')
			将所有包含这个正则表达式的href全部获取到返回
			links.extract_links(response)进行查看提取到的链接
			【注】将重复的url去除掉
		通过xpath提取
			links = LinkExtractor(restrict_xpaths='//ul[@class="pagination pagination-sm"]/li/a')
		通过css提取
			links = LinkExtractor(restrict_css='.pagination > li > a')
	使用
		调度器有去重的功能，只要包含了，就可以爬取所有页面

### 分布式爬虫

1、存储到mysql、mongodb
	通过crawlspider经常爬取这些有分页、有详情页的
	读取settings文件中的参数:
		from scrapy.utils.project import get_project_settings
		settings = get_project_settings()
	custom_settings
		# 自己定制配置文件中的某些选项
	    custom_settings = {
	        "ITEM_PIPELINES": {
	            'movieproject.pipelines.MyMongoDbPipeline': 302,
	        }
	    }
2、redis简单回顾
	配置
	windows redis启动
		redis-server.exe redis.windows.conf
	linux redis客户端连接windows的服务器
		redis-cli -h 10.8.153.5
	配置windows的redis服务器可以让其它客户端连接和读写
		第56行，把这个注释掉
			#bind 127.0.0.1
		第75行
			protected-mode no
3、分布式部署
	scrapy ： 一个框架，不能实现分布式爬取
	scrapy-redis ： 基于这个框架开发的一套组件，可以让scrapy实现分布式的爬取
	（1）安装： pip install scrapy-redis
	（2）样本查看
		https://github.com/rmax/scrapy-redis
		example-project\example\spiders
		dmoz.py : 普通crawlspider，没有参考价值
		myspider_redis.py ： 分布式的Spider模板
		mycrawler_redis.py ： 分布式的CrawlSpider模板
		Spider       ====》  RedisSpider
		CrawlSpider  ====》  RedisCrawlSpider
		name                 name
		redis_key            start_urls
		__init__()           allowed_domains
		【注】__init__()是一个坑，现在还是使用allowed_domains这种列表的形式
	（3）存储到redis中
		scrapy-redis组件已经写好往redis中存放的管道，只需要使用即可，默认存储到本机的redis服务中
		如果想存储到其它的redis服务中，需要在配置文件中配置
		REDIS_HOST = 'ip地址'
		REDIS_PORT = 6379
	（4）部署分布式
		爬虫文件按照模板文件修改
		配置文件中添加
			# 使用scrapy-redis组件的去重队列
			DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
			# 使用scrapy-redis组件自己的调度器
			SCHEDULER = "scrapy_redis.scheduler.Scheduler"
			# 是否允许暂停
			SCHEDULER_PERSIST = True
		【注】分布式爬取的时候，指令不是scrapy crawl xx
		scrapy runspider xxx.py
		在我的windows中往队列中添加起始url
			lpush nnspider:start_urls "http://www.xxxxx.com/movie/